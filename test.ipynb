{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def linear_anneal(t, T, start, final, percentage):\n",
    "    ''' Linear annealing scheduler\n",
    "    t: current timestep\n",
    "    T: total timesteps\n",
    "    start: initial value\n",
    "    final: value after percentage*T steps\n",
    "    percentage: percentage of T after which annealing finishes\n",
    "    '''\n",
    "    final_from_T = int(percentage * T)\n",
    "    if t > final_from_T:\n",
    "        return final\n",
    "    else:\n",
    "        return final + (start - final) * (final_from_T - t) / final_from_T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0.1585"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_anneal(17, 25, 1.0, 0.01, 0.8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def build_nn():\n",
    "    input_size = 72\n",
    "    output = 8\n",
    "    layer_sizes = [input_size] + [128, 128, 128] + [output]\n",
    "\n",
    "    assert len(layer_sizes) > 1\n",
    "    layers = []\n",
    "    for index in range(len(layer_sizes) - 1):\n",
    "        linear = nn.Linear(layer_sizes[index], layer_sizes[index + 1])\n",
    "        act = nn.ReLU() if index < len(layer_sizes) - 2 else nn.Identity()\n",
    "        layers += (linear, act)\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 1., 0., 0.]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\n",
    "    'C:/Users/evani/OneDrive/AI leiden/Sanquin/RL_matching-main/NN training data/reg_ABDCcEeKkFyaFybJkaJkbMNSs/Q_matrices_0_0.pickle',\n",
    "    allow_pickle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "f = 'C:/Users/evani/OneDrive/AI leiden/Sanquin/NN training data/NN training data/reg_ABDCcEeKkFyaFybJkaJkbMNSs/q_matrices/Q_matrices_0_0.pickle'\n",
    "s = 'C:/Users/evani/OneDrive/AI leiden/Sanquin/NN training data/NN training data/reg_ABDCcEeKkFyaFybJkaJkbMNSs/states/states_0_0.pickle'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "objects = []\n",
    "with (open(f, 'rb')) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            objects.append((pickle.load(openfile)))\n",
    "        except EOFError:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 1., 0., 0.]])"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(f, allow_pickle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m state0 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(s, allow_pickle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "state0 = np.load(s, allow_pickle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "image = state0[0].reshape(8, 9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[23.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [28.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 2.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [37.,  0.,  0.,  0.,  0.,  0.,  0., 14.,  1.],\n       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state0[3000].reshape(8, 9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 540x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.matshow(state0[3000].reshape(8, 9))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "layers = [128, 128, 128]\n",
    "input_size = 72\n",
    "output = 8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def build_nn():\n",
    "    input_size = [22]\n",
    "    output = [8]\n",
    "    layer_sizes = input_size + [128, 128, 128] + output\n",
    "\n",
    "    assert len(layer_sizes) > 1\n",
    "    layers = []\n",
    "    for index in range(len(layer_sizes) - 1):\n",
    "        linear = nn.Linear(layer_sizes[index], layer_sizes[index + 1])\n",
    "        act = nn.ReLU() if index < len(layer_sizes) - 2 else nn.Identity()\n",
    "        layers += (linear, act)\n",
    "    return nn.Sequential(*layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m build_nn()\n",
      "Cell \u001B[1;32mIn[2], line 9\u001B[0m, in \u001B[0;36mbuild_nn\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m layers \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(layer_sizes) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m----> 9\u001B[0m     linear \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(layer_sizes[index], layer_sizes[index \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     10\u001B[0m     act \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mReLU() \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(layer_sizes) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m nn\u001B[38;5;241m.\u001B[39mIdentity()\n\u001B[0;32m     11\u001B[0m     layers \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (linear, act)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "model = build_nn()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x80 and 72x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[156], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model(torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m80\u001B[39m))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\sanquin_py3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\sanquin_py3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\sanquin_py3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\sanquin_py3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1x80 and 72x128)"
     ]
    }
   ],
   "source": [
    "model(torch.rand(80))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=72, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (7): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "[72, 8, 128, 128]"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[72] + [8] + [128, 128]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print\n",
    "        name, param.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "          0\ncolumn  500",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>column</th>\n      <td>500</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([500], ['column'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    n_layers = 3\n",
    "    layers = []\n",
    "\n",
    "    in_features = 72\n",
    "    for i in range(n_layers):\n",
    "        out_features = random.randint(72, 256)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, 8))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "model = define_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=72, out_features=135, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=135, out_features=156, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=156, out_features=74, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=74, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    layers = []\n",
    "\n",
    "    in_features = 72\n",
    "    non = [128, 128, 128]\n",
    "    p = [0.34, 0.66, 0.11]\n",
    "    output = 8\n",
    "    for i in range(len(non) - 1):\n",
    "        out_features = non[i]\n",
    "        act = nn.ReLU()\n",
    "        linear = nn.Linear(in_features, out_features)\n",
    "        layers += (linear, act)\n",
    "        layers.append(nn.Dropout(p[i]))\n",
    "        in_features = out_features\n",
    "    layers.append((nn.Linear(in_features, output)))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "model = define_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=72, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.34, inplace=False)\n",
      "  (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.66, inplace=False)\n",
      "  (6): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'results/kickstart/1/loss_1.csv'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'results/kickstart/{}/loss_1.csv'.format(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.5296, 0.8692, 0.7453, 0.3743],\n        [0.9395, 0.7282, 0.1014, 0.0806],\n        [0.3397, 0.8241, 0.5919, 0.1700],\n        [0.4197, 0.6870, 0.4039, 0.1415],\n        [0.6582, 0.0717, 0.1942, 0.7462]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.rand(5, 4, requires_grad=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.3514)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "pred = torch.Tensor([0.2, 0.7, 0.1])\n",
    "true = torch.Tensor([1, 0, 0])\n",
    "\n",
    "loss(true, pred)\n",
    "# tensor(0.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "demand = pd.read_csv('C:/Users/evani/OneDrive/AI leiden/Sanquin/RL_matching-main/supply/147300/cau100_afr0_asi0_0.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "        A  B  D  C  c  E  e  K  k  M  N  S  s  Fya  Fyb  Jka  Jkb  Ethnicity  \\\n3       1  0  1  1  1  0  1  0  1  1  1  1  1    0    1    1    0  Caucasian   \n6       1  0  1  1  1  0  1  0  1  1  1  0  1    1    1    0    1  Caucasian   \n12      1  0  1  0  1  1  1  0  1  1  1  1  1    1    0    1    1  Caucasian   \n13      1  0  1  1  1  0  1  0  1  1  0  1  0    0    1    1    0  Caucasian   \n17      1  0  1  1  1  0  1  0  1  1  1  1  1    1    1    0    1  Caucasian   \n...    .. .. .. .. .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...        ...   \n147268  1  0  1  0  1  1  1  0  1  1  1  1  0    0    1    1    1  Caucasian   \n147269  1  0  1  1  1  1  1  0  1  1  1  0  1    1    1    1    1  Caucasian   \n147276  1  0  1  1  0  0  1  0  1  1  1  1  1    1    1    0    1  Caucasian   \n147277  1  0  1  1  1  1  1  0  1  0  1  1  1    1    1    1    1  Caucasian   \n147282  1  0  1  1  1  0  1  0  1  0  1  1  1    0    1    1    1  Caucasian   \n\n         Index  \n3            3  \n6            6  \n12          12  \n13          13  \n17          17  \n...        ...  \n147268  147268  \n147269  147269  \n147276  147276  \n147277  147277  \n147282  147282  \n\n[47254 rows x 19 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n      <th>D</th>\n      <th>C</th>\n      <th>c</th>\n      <th>E</th>\n      <th>e</th>\n      <th>K</th>\n      <th>k</th>\n      <th>M</th>\n      <th>N</th>\n      <th>S</th>\n      <th>s</th>\n      <th>Fya</th>\n      <th>Fyb</th>\n      <th>Jka</th>\n      <th>Jkb</th>\n      <th>Ethnicity</th>\n      <th>Index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Caucasian</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Caucasian</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Caucasian</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Caucasian</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Caucasian</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>147268</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Caucasian</td>\n      <td>147268</td>\n    </tr>\n    <tr>\n      <th>147269</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Caucasian</td>\n      <td>147269</td>\n    </tr>\n    <tr>\n      <th>147276</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Caucasian</td>\n      <td>147276</td>\n    </tr>\n    <tr>\n      <th>147277</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Caucasian</td>\n      <td>147277</td>\n    </tr>\n    <tr>\n      <th>147282</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Caucasian</td>\n      <td>147282</td>\n    </tr>\n  </tbody>\n</table>\n<p>47254 rows × 19 columns</p>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demand.loc[(demand['A'] == 1) & (demand['D'] == 1)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    layers = []\n",
    "    nnl = [64, 128, 64]\n",
    "    in_features = 8 * 3\n",
    "    output = 8\n",
    "    for i in range(len(nnl)):\n",
    "        out_features = nnl[i]\n",
    "        act = nn.ReLU()\n",
    "        linear = nn.Linear(in_features, out_features)\n",
    "        layers += (linear, act)\n",
    "        # layers.append(nn.Dropout(self.p[i]))\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, output))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "model = define_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[98], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m8\u001B[39m)\n\u001B[0;32m      2\u001B[0m target \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m----> 3\u001B[0m F\u001B[38;5;241m.\u001B[39mcross_entropy(\u001B[38;5;28minput\u001B[39m, target)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\sanquin_py3\\Lib\\site-packages\\torch\\nn\\functional.py:3029\u001B[0m, in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   3027\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3028\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3029\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mcross_entropy_loss(\u001B[38;5;28minput\u001B[39m, target, weight, _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "input = torch.rand(8)\n",
    "target = torch.tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
    "F.cross_entropy(input, target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.6453, grad_fn=<DivBackward1>)"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of target with class probabilities\n",
    "torch.manual_seed(10)\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "F.cross_entropy(input, target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([4, 4, 4])"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3, dtype=torch.long).random_(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0963,  1.5378,  0.2506,  0.3940,  0.8108],\n        [ 0.6037,  0.4361,  0.0890,  0.1018, -1.3361],\n        [ 0.1913,  1.2856,  0.9063, -0.5357,  0.8521]], requires_grad=True)"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 5, requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0634, 0.2335, 0.4376, 0.0839, 0.1816],\n        [0.3771, 0.2034, 0.2014, 0.0934, 0.1247],\n        [0.1015, 0.1923, 0.3651, 0.2213, 0.1197]])"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 5).softmax(dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "class Q_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input, output, nn):\n",
    "        super().__init__()\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        self.nn = nn\n",
    "        # self.p = p\n",
    "        self.model = self.define_model()\n",
    "\n",
    "    def define_model(self):\n",
    "        layers = []\n",
    "\n",
    "        in_features = self.input\n",
    "        for i in range(len(self.nn)):\n",
    "            out_features = self.nn[i]\n",
    "            act = nn.ReLU()\n",
    "            linear = nn.Linear(in_features, out_features)\n",
    "            layers += (linear, act)\n",
    "            # layers.append(nn.Dropout(self.p[i]))\n",
    "            in_features = out_features\n",
    "        layers.append(nn.Linear(in_features, self.output))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/kickstart/10/model_4380.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[430], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m Q_net(\u001B[38;5;241m24\u001B[39m, \u001B[38;5;241m8\u001B[39m, [\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m64\u001B[39m])\u001B[38;5;241m.\u001B[39mmodel\n\u001B[1;32m----> 2\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels/kickstart/10/model_4380.pt\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      4\u001B[0m model\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      5\u001B[0m model\u001B[38;5;241m.\u001B[39mshare_memory()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\sanquin_py3\\Lib\\site-packages\\torch\\serialization.py:791\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001B[0m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    789\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 791\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _open_file_like(f, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    793\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m    794\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m    795\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m    796\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\sanquin_py3\\Lib\\site-packages\\torch\\serialization.py:271\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 271\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _open_file(name_or_buffer, mode)\n\u001B[0;32m    272\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    273\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\sanquin_py3\\Lib\\site-packages\\torch\\serialization.py:252\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 252\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mopen\u001B[39m(name, mode))\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'models/kickstart/10/model_4380.pt'"
     ]
    }
   ],
   "source": [
    "model = Q_net(24, 8, [64, 128, 64]).model\n",
    "model.load_state_dict(torch.load(\n",
    "    'models/kickstart/10/model_400.pt'))\n",
    "model.to('cpu')\n",
    "model.share_memory()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-15306.2539, -13245.0283, -20370.7539, -18368.1523, -16832.1680,\n        -14149.8408, -21989.2402, -21389.0059], grad_fn=<AddBackward0>)"
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(24))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "outputs": [],
   "source": [
    "targets = 'NN training data/1_1/q-matrices/Q_matrices_4_1.pickle'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "outputs": [],
   "source": [
    "array = np.load(targets, allow_pickle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "outputs": [],
   "source": [
    "zeros = np.zeros((1, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.4518, -0.7648, -0.8108, -0.4696,  0.2031],\n        [ 0.7067,  0.2936, -0.5483, -0.4325,  0.6899],\n        [ 0.8358,  0.2872,  0.1987, -1.3307,  0.0143]], requires_grad=True)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 5, requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1., 0., 1., 0., 0., 0.],\n",
    "                   [1., 0., 0., 0., 0., 0.],\n",
    "                   [1., 0., 0., 0., 0., 0.]]).int()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3, 0, 1, 0, 0, 0])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t1, dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}